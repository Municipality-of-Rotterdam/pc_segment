"""Batch segmentation of objects in point cloud raster tiles using SAM.

This script:
1. Reads the rasterized point cloud tiles and point prompts generated by `pc_prep`.
2. Runs the Segment Anything Model (SAM) on those prompts.
3. Writes out per-tile segmentation masks (`pred_mask.png`).
4. Produces a JSON manifest that maps each tile to its segmentation output.

Intended workflow:
    pc_prep.prep_pc.py  --> generates raster images + prompt JSON per tile
    pc_segment.segment_pc.py  --> consumes those, runs SAM, outputs masks
"""

import argparse
import json
import os

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
from segment_anything import SamPredictor, sam_model_registry
from segment_anything.utils.transforms import ResizeLongestSide
from tqdm import tqdm

from pc_segment.helper_functions import get_collection_info
from pc_segment.logger import logger


def show_points(coords: np.ndarray, ax: plt.Axes, marker_size: int = 550) -> None:
    """Plot interactive prompt points on an axes.

    Parameters
    ----------
    coords : numpy.ndarray
        Array of shape (N, 2) with pixel coordinates [x, y] for each prompt.
    ax : matplotlib.axes.Axes
        Axis on which to draw.
    marker_size : int, default 550
        Marker size for visualization.
    """
    ax.scatter(
        coords[:, 0],
        coords[:, 1],
        color="red",
        marker="*",
        s=marker_size,
        edgecolor="white",
        linewidth=1.25,
    )


def save_image_prompt(
    image_path: str,
    point_coords: list[list[int]] | list[tuple[int, int]],
    output_path: str,
) -> None:
    """Save a debug image with the raster background and the SAM click prompts overlaid.

    Parameters
    ----------
    image_path : str
        Path to the GeoTIFF/PNG raster produced during preprocessing.
        This is assumed to be an RGB rasterization of the point cloud tile.
    point_coords : list[list[int]] | list[tuple[int, int]]
        List of pixel coordinates that were used as positive prompts.
        Shape: [[x0, y0], [x1, y1], ...]
    output_path : str
        Where to save the debug visualization (PNG).
    """
    plt.figure(figsize=(20, 20))

    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.imshow(img)
    show_points(np.array(point_coords), plt.gca())
    plt.savefig(output_path)
    plt.close()

    logger.info("Saved prompt-debug visualization to %s", output_path)


def prepare_image(
    image: np.ndarray, transform: ResizeLongestSide, device: torch.device
) -> torch.Tensor:
    """Resize and convert an RGB image into a SAM-ready tensor.

    Parameters
    ----------
    image : numpy.ndarray
        Image array (H, W, 3) in RGB uint8 format.
    transform : segment_anything.utils.transforms.ResizeLongestSide
        SAM-provided resizing transform that resizes while preserving aspect ratio.
    device : torch.device
        The torch device on which inference will run ("cuda" or "cpu").

    Returns:
    -------
    torch.Tensor
        Tensor shaped (3, H_resized, W_resized) on the correct device.
    """
    image = transform.apply_image(image)
    image = torch.as_tensor(image, device=device)
    return image.permute(2, 0, 1).contiguous()


def prompt_segment(
    predictor: SamPredictor,
    image_paths: list[str],
    point_prompts: list[list[list[int]]],
    point_labels: list[list[list[int]]],
    outfnames: list[str],
    debug: bool = False,
) -> list[np.ndarray]:
    """Run SAM on a batch of images and write predicted segmentation masks to disk.

    For each image:
    - Loads the raster produced by `pc_prep`.
    - Applies point prompts + labels.
    - Runs the SAM model.
    - Combines the per-prompt masks into a single 2D label image.
    - Saves that mask to <segment_dir>/<basename>/<pc_code>/pred_mask.png.

    If `debug` is True, a colored heatmap of the segmentation is also saved.

    Parameters
    ----------
    predictor : SamPredictor
        A SAM predictor initialized with a model checkpoint.
    image_paths : list[str]
        Paths to rasterized point cloud tiles (RGB images).
    point_prompts : list[list[list[int]]]
        Per-image list of prompt coordinates (pixel space).
        Example shape for one item: [[x0, y0], [x1, y1], ...].
    point_labels : list[list[list[int]]]
        Per-image list of labels (0/1 per prompt).
        Shape mirrors `point_prompts`.
        Typical usage: all 1s = positive tree clicks.
    outfnames : list[str]
        Paths where each mask image should be saved.
        Must align 1:1 with `image_paths`.
    debug : bool, default False
        If True, saves a debug mask visualization (JET colormap).

    Returns:
    -------
    list[numpy.ndarray]
        List of combined mask arrays (uint16-ish style labels),
        one per input image.
    """
    resize_transform = ResizeLongestSide(predictor.model.image_encoder.img_size)

    batched_input = []
    for img_path, points, labels in zip(
        image_paths, point_prompts, point_labels, strict=False
    ):
        # Load & RGB-convert source image
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        processed_image = prepare_image(img, resize_transform, predictor.device)

        input_points = torch.tensor(points, device=predictor.device)
        input_labels = torch.tensor(labels, device=predictor.device)

        # Transform click coords to resized coordinate frame
        transformed_points = resize_transform.apply_coords_torch(
            input_points, img.shape[:2]
        )

        batched_input.append(
            {
                "image": processed_image,
                "point_coords": transformed_points.reshape(len(points), 1, 2),
                "point_labels": input_labels,
                "original_size": img.shape[:2],
            }
        )

    # Run SAM forward pass in one go
    batched_output = predictor.model(batched_input, multimask_output=False)

    masks_list: list[np.ndarray] = []
    for idx, output in enumerate(batched_output):
        masks = output["masks"]  # [num_prompts, 1, H, W] boolean-ish tensor
        maskimg = combine_masks(masks)

        if debug:
            # Produce a human-friendly (visually colored) version too
            # Scale to 0-255 and colorize
            max_val = max([1, maskimg.max()])
            maskimg_scaled = (maskimg * (255 / max_val)).astype(np.uint8)
            maskimg_colored_scaled = cv2.applyColorMap(maskimg_scaled, cv2.COLORMAP_JET)

            outfname_debug = outfnames[idx].replace(".png", "_debug.png")
            cv2.imwrite(outfname_debug, maskimg_colored_scaled)
            logger.info("Debug segmented image written to %s", outfname_debug)

        # Write raw mask (uint16/uint8 labels per pixel)
        cv2.imwrite(outfnames[idx], maskimg)
        logger.info("Segmented mask written to %s", outfnames[idx])

        masks_list.append(maskimg)

    return masks_list


def initialize_model(model_path: str) -> SamPredictor:
    """Load a SAM checkpoint and return a ready-to-use predictor.

    Parameters
    ----------
    model_path : str
        Path to the SAM .pth checkpoint file (e.g. ViT-H weights).

    Returns:
    -------
    SamPredictor
        Predictor object wrapping the loaded SAM model on CPU or GPU.
    """
    MODEL_TYPE = "vit_h"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    sam = sam_model_registry[MODEL_TYPE](checkpoint=model_path)
    sam.to(device=DEVICE)

    predictor = SamPredictor(sam)
    return predictor


def combine_masks(masks: torch.Tensor) -> np.ndarray:
    """Merge multiple binary masks (one per prompt) into a single labeled mask.

    The SAM model returns a stack of masks, one for each input point.
    We convert that stack into a single 2D array where:
    - pixels belonging to prompt i get label (i + 1),
    - background pixels get label 0.

    Parameters
    ----------
    masks : torch.Tensor
        SAM output of shape (C, B, H, W), where:
        - C = number of prompts
        - B = batch size (should be 1 for each element here)
        - H, W = spatial size

    Returns:
    -------
    numpy.ndarray
        A 2D array (H, W) of integer labels.
    """
    bool_mask = masks.cpu().numpy()  # shape: (C, B, H, W), bool-ish
    # Take argmax over C (prompt dimension) -> which prompt "wins"
    max_indices = np.argmax(bool_mask, axis=0) + 1  # shape: (B, H, W)

    # Wherever *all* prompts are False, label becomes 0
    all_false_mask = ~np.any(bool_mask, axis=0)  # shape: (B, H, W)
    max_indices[all_false_mask] = 0

    # Remove batch dim B (which should be 1)
    maskimg = max_indices.squeeze(axis=0)
    return maskimg


def segment_batch_items(
    keys: list[str],
    preprocessing_list: list[dict[str, str]],
    img_dir: str,
    segment_dir: str,
    predictor: SamPredictor,
    debug: bool = False,
    batch_size: int = 2,
    overwrite: bool = False,
) -> dict[str, str]:
    """Run SAM segmentation for a batch of tiles and write results to disk.

    For each tile:
    - Loads raster image and prompt JSON (`point_prompt.json`) from `img_dir`.
    - Runs SAM.
    - Writes `<segment_dir>/<basename>/<pc_code>/pred_mask.png`.
    - Optionally writes `<...>/debug_prompt_image.png` with click points.
    - Updates an in-memory index:
        { "<tile_id>": "<rel_path_to_pred_mask.png>" }

    The function batches calls to `prompt_segment()` for efficiency.

    Parameters
    ----------
    keys : list[str]
        Tile identifiers. Typically these are the original point cloud paths
        used as keys in `img_metadata.json`.
    preprocessing_list : list[dict[str, str]]
        Metadata entries (same order as `keys`) from `img_metadata.json`. Each
        entry must contain at least "img_path" and "prompt_path" (relative to img_dir).
    img_dir : str
        Root folder containing the raster images and prompt JSON files generated by preprocessing.
    segment_dir : str
        Root folder where segmentation outputs should be stored.
        Per-tile subdirectories are created automatically.
    predictor : SamPredictor
        A loaded model predictor from `initialize_model()`.
    debug : bool, default False
        If True, also writes `debug_prompt_image.png` (image + prompt clicks).
    batch_size : int, default 2
        Mini-batch size for calling SAM.
    overwrite : bool, default False
        If False, skip tiles that already have `pred_mask.png`.

    Returns:
    -------
    dict[str, str]
        Mapping:
            { <tile_key>: <relative_path_under_segment_dir_to_pred_mask.png> }
    """
    assert len(keys) == len(
        preprocessing_list
    ), "keys and preprocessing_list must align"

    out_mapping: dict[str, str] = {}

    # Accumulators for a mini-batch call to SAM
    img_paths: list[str] = []
    point_prompts: list[list[list[int]]] = []
    point_labels: list[list[list[int]]] = []
    outfnames: list[str] = []
    pc_paths_for_batch: list[str] = []

    def _flush_batch() -> None:
        """Run segmentation on current batch and record outputs in out_mapping."""
        if not img_paths:
            return

        prompt_segment(
            predictor=predictor,
            image_paths=img_paths,
            point_prompts=point_prompts,
            point_labels=point_labels,
            outfnames=outfnames,
            debug=debug,
        )

        for i, pc_path in enumerate(pc_paths_for_batch):
            rel = outfnames[i].replace(os.path.join(segment_dir, ""), "")
            out_mapping[pc_path] = rel

        # Clear accumulators for next mini-batch
        img_paths.clear()
        point_prompts.clear()
        point_labels.clear()
        outfnames.clear()
        pc_paths_for_batch.clear()

    for key, prep in zip(keys, preprocessing_list, strict=False):
        basename = os.path.basename(os.path.splitext(key)[0])
        pc_code = get_collection_info(pc_filename=key)

        # Output location for this tile's segmentation
        outdirname = os.path.join(segment_dir, basename, pc_code)
        os.makedirs(outdirname, exist_ok=True)

        # Resolve input raster + prompt file paths from `img_metadata.json`
        image_path = os.path.join(img_dir, prep["img_path"])
        prompt_rel = prep["prompt_path"]
        prompt_path = (
            os.path.join(img_dir, prompt_rel)
            if prompt_rel
            else os.path.join(os.path.dirname(image_path), "point_prompt.json")
        )

        mask_out = os.path.join(outdirname, "pred_mask.png")

        # Reuse existing result if present and overwrite=False
        if os.path.exists(mask_out) and not overwrite:
            logger.info("Skipping %s â€” output already exists.", mask_out)
            out_mapping[key] = mask_out.replace(os.path.join(segment_dir, ""), "")
            continue

        # Load SAM prompts (clicks and their labels)
        with open(prompt_path) as f:
            prompt_dict = json.load(f)
        pts = prompt_dict.get("point_coords", [])
        lbl = prompt_dict.get("point_labels", [])

        if debug:
            dbg = os.path.join(outdirname, "debug_prompt_image.png")
            save_image_prompt(image_path, pts, dbg)

        # Add to current batch
        img_paths.append(image_path)
        point_prompts.append(pts)
        point_labels.append(lbl)
        outfnames.append(mask_out)
        pc_paths_for_batch.append(key)

        # Flush when batch is full
        if len(img_paths) == batch_size:
            _flush_batch()

    # Flush any trailing items smaller than batch_size
    _flush_batch()

    return out_mapping


def segment_pointcloud_img(args: argparse.Namespace) -> dict[str, str]:
    """High-level runner: iterate through all tiles in img_metadata, segment them, and
    write a final JSON manifest describing all segmentation outputs.

    Parameters
    ----------
    args : argparse.Namespace
        Parsed CLI arguments from `configure_arg_parser()`.

    Returns:
    -------
    dict[str, str]
        Mapping:
            { <tile_key>: <relative_output_mask_path> }
        This is also saved to `args.segment_metadata`.

    Side Effects
    ------------
    - Loads SAM once and reuses it for all tiles.
    - Writes per-tile outputs into `segment_dir`.
    - Writes `segment_metadata.json`.
    - Displays a tqdm progress bar.
    """
    # Load the metadata created by pc_prep (img_metadata.json)
    with open(args.img_metadata) as f:
        img_metadata: dict[str, dict[str, str]] = json.load(f)

    keys = list(img_metadata.keys())
    total = len(keys)

    predictor = initialize_model(model_path=args.model_path)
    logger.info(
        "Running segmentation over %d items (batch_size=%s).", total, args.batch_size
    )

    segment_metadata: dict[str, str] = {}

    with tqdm(total=total, desc="Segmenting images", unit="img") as pbar:
        for i in range(0, total, args.batch_size):
            chunk_keys = keys[i : i + args.batch_size]
            chunk_preps = [img_metadata[k] for k in chunk_keys]

            mapping = segment_batch_items(
                keys=chunk_keys,
                preprocessing_list=chunk_preps,
                img_dir=args.img_dir,
                segment_dir=args.segment_dir,
                predictor=predictor,
                debug=getattr(args, "debug", False),
                batch_size=args.batch_size,
                overwrite=args.overwrite,
            )

            segment_metadata.update(mapping)
            pbar.update(len(chunk_keys))

    # Persist the final global segmentation index
    os.makedirs(os.path.dirname(args.segment_metadata), exist_ok=True)
    with open(args.segment_metadata, "w") as f:
        json.dump(segment_metadata, f, indent=4)

    logger.info(
        "Segmented %d/%d items. Metadata -> %s",
        len(segment_metadata),
        total,
        args.segment_metadata,
    )

    return segment_metadata


def configure_arg_parser() -> argparse.Namespace:
    """Build and parse CLI arguments for segmentation.

    Returns:
    -------
    argparse.Namespace
        Parsed CLI namespace with the following fields:

    Required
    --------
    --img_metadata : str
        Path to the `img_metadata.json` file produced by `pc_prep`.
        This file maps each tile to its raster path and prompt path.
    --img_dir : str
        Root directory containing those rasters and prompt JSONs.
    --model_path : str
        Path to the SAM checkpoint (.pth).
    --segment_dir : str
        Directory where segmentation outputs will be written.
        Per-tile subdirectories are created automatically.
    --segment_metadata : str
        Path to write a final JSON manifest mapping tile -> segmentation output.

    Optional
    --------
    --batch_size : int, default 2
        Number of tiles to send to SAM at once.
    --overwrite : bool, default False
        If True, recompute masks even if pred_mask.png already exists.
    --debug : bool, default False
        If True, also writes: debug_prompt_image.png and *_debug.png masks.
    """
    parser = argparse.ArgumentParser(
        description="Run SAM segmentation on rasterized point cloud tiles."
    )
    parser.add_argument(
        "--img_metadata",
        type=str,
        help="Path to img_metadata.json created by preprocessing.",
    )
    parser.add_argument(
        "--img_dir",
        type=str,
        help="Directory containing rasters + prompt JSONs from preprocessing.",
    )
    parser.add_argument(
        "--model_path", type=str, help="Path to the SAM model checkpoint (.pth)."
    )
    parser.add_argument(
        "--segment_dir",
        type=str,
        help="Directory where predicted masks will be written.",
    )
    parser.add_argument(
        "--segment_metadata",
        type=str,
        help="Output JSON mapping each tile to its segmentation mask path.",
    )
    parser.add_argument(
        "--batch_size", type=int, default=2, help="Batch size for SAM inference."
    )
    parser.add_argument(
        "--overwrite",
        type=lambda s: s.lower() == "true",
        default=False,
        help="If 'true', recompute even if mask already exists.",
    )
    parser.add_argument(
        "--debug",
        type=lambda s: s.lower() == "true",
        default=False,
        help="If 'true', save debug overlay images and colorized masks.",
    )
    return parser.parse_args()


def main() -> None:
    """CLI entry point.

    Steps:
    1. Parse arguments.
    2. Run segmentation over all tiles in img_metadata.
    3. Write segment_metadata.json describing outputs.
    """
    args = configure_arg_parser()
    segment_pointcloud_img(args=args)


if __name__ == "__main__":
    main()
